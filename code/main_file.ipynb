{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stories.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for story in reader:\n",
    "        stories.append({'story_id': story[1], 'title': story[2], 'english_title': story[3], 'reading_level_updated': story[4], 'story_language': story[5], 'synopsis': story[6], 'content': story[7], 'category_name': story[8], 'tag_name': story[9], 'story_original_title': story[10]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stories currently has all stories through all languages. Now, taking only stories in Hindi language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_hindi = []\n",
    "for each in stories:\n",
    "    if each['story_language']=='Hindi':\n",
    "        stories_hindi.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tokenizer based on rules written by LTRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusImporter('hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.sentence import TokenizeSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Hindi tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_tokenizer = TokenizeSentence('hindi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'पहलवान जी।  दस किलो दूध पी जाते।  पचास रोटी खाते।  100 किलो वज़न उठाते।  खूब अकड़ कर चलते।  सब बच्चों पर हुक्म चलाते।   एक दिन गप्पू बोला, “मुझसे कुश्ती लड़ोगे?” \\u2003 पहलवान हँसा। फिर मान गया।  दोनों ने ताल ठोंकी।  पहलवान ने गप्पू को दबोच लिया।  गप्पू ने उसके पेट में गुदगुदी लगायी।  ही...ही...ही... पहलवान उछल पड़ा।  गप्पू उसे नचाने लगा।  पहलवान की हालत खराब। मैदान छोड़ भाग लिया।  अब बच्चे उससे नहीं डरते। वह बच्चों से डरता है। '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_hindi[6]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_tokenized = hindi_tokenizer.tokenize(stories_hindi[6]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['पहलवान',\n",
       " 'जी',\n",
       " '।',\n",
       " 'दस',\n",
       " 'किलो',\n",
       " 'दूध',\n",
       " 'पी',\n",
       " 'जाते',\n",
       " '।',\n",
       " 'पचास',\n",
       " 'रोटी',\n",
       " 'खाते',\n",
       " '।',\n",
       " '100',\n",
       " 'किलो',\n",
       " 'वज़न',\n",
       " 'उठाते',\n",
       " '।',\n",
       " 'खूब',\n",
       " 'अकड़',\n",
       " 'कर',\n",
       " 'चलते',\n",
       " '।',\n",
       " 'सब',\n",
       " 'बच्चों',\n",
       " 'पर',\n",
       " 'हुक्म',\n",
       " 'चलाते',\n",
       " '।',\n",
       " 'एक',\n",
       " 'दिन',\n",
       " 'गप्पू',\n",
       " 'बोला',\n",
       " ',',\n",
       " '“मुझसे',\n",
       " 'कुश्ती',\n",
       " 'लड़ोगे',\n",
       " '?',\n",
       " '”',\n",
       " '\\u2003',\n",
       " 'पहलवान',\n",
       " 'हँसा',\n",
       " '।',\n",
       " 'फिर',\n",
       " 'मान',\n",
       " 'गया',\n",
       " '।',\n",
       " 'दोनों',\n",
       " 'ने',\n",
       " 'ताल',\n",
       " 'ठोंकी',\n",
       " '।',\n",
       " 'पहलवान',\n",
       " 'ने',\n",
       " 'गप्पू',\n",
       " 'को',\n",
       " 'दबोच',\n",
       " 'लिया',\n",
       " '।',\n",
       " 'गप्पू',\n",
       " 'ने',\n",
       " 'उसके',\n",
       " 'पेट',\n",
       " 'में',\n",
       " 'गुदगुदी',\n",
       " 'लगायी',\n",
       " '।',\n",
       " 'ही',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'ही',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'ही',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'पहलवान',\n",
       " 'उछल',\n",
       " 'पड़ा',\n",
       " '।',\n",
       " 'गप्पू',\n",
       " 'उसे',\n",
       " 'नचाने',\n",
       " 'लगा',\n",
       " '।',\n",
       " 'पहलवान',\n",
       " 'की',\n",
       " 'हालत',\n",
       " 'खराब',\n",
       " '।',\n",
       " 'मैदान',\n",
       " 'छोड़',\n",
       " 'भाग',\n",
       " 'लिया',\n",
       " '।',\n",
       " 'अब',\n",
       " 'बच्चे',\n",
       " 'उससे',\n",
       " 'नहीं',\n",
       " 'डरते',\n",
       " '।',\n",
       " 'वह',\n",
       " 'बच्चों',\n",
       " 'से',\n",
       " 'डरता',\n",
       " 'है',\n",
       " '।']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, each in enumerate(stories_hindi):\n",
    "    stories_hindi[index]['tok_content'] = hindi_tokenizer.tokenize(stories_hindi[index]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_levels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, each in enumerate(stories_hindi):\n",
    "    reading_levels[each['reading_level_updated']] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': [], 'L2': [], 'L3': [], 'L4': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reading_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the number of reading levels - in case it's not fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels = len(reading_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in stories_hindi:\n",
    "    reading_levels[each['reading_level_updated']].append(each['tok_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to see what differentiates each difficulty level from the other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly, what are the most frequently occuring tokens in each difficulty level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "98659\n",
      "303\n",
      "171488\n",
      "137\n",
      "199805\n",
      "88\n",
      "251993\n"
     ]
    }
   ],
   "source": [
    "all_counts = {}\n",
    "for index, each in enumerate(reading_levels):\n",
    "    all_tokens = []\n",
    "    print(len(reading_levels[each]))\n",
    "    for each1 in reading_levels[each]:\n",
    "        all_tokens = all_tokens + each1\n",
    "    print(len(all_tokens))\n",
    "    #all_counts[each]=nltk.FreqDist(nltk.ngrams(all_tokens, len(all_tokens)))\n",
    "    all_counts[each]=all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98659"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_counts['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for each in all_counts:\n",
    "    all_counts[each] = Counter(all_counts[each])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case it was hard to follow, what has been done till now is that for each difficulty level - L1, L2, L3, and L4 - we have found the count for each word. Nowe, we can see what the top 'x' words are for each - this way we're checking the difficulty of lexical items that appear at each level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(';', 9882),\n",
       " ('&', 9251),\n",
       " ('nbsp', 9227),\n",
       " ('.', 3431),\n",
       " ('-', 2671),\n",
       " ('।', 2611),\n",
       " ('है', 1698),\n",
       " (',', 1644),\n",
       " ('!', 1180),\n",
       " ('\"', 997)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counts['L1'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(';', 9882), ('&', 9251), ('nbsp', 9227), ('.', 3431), ('-', 2671), ('।', 2611), ('है', 1698), (',', 1644), ('!', 1180), ('\"', 997)]\n",
      "xxxxxxxxxxxxxx\n",
      "[(';', 9684), ('&', 8693), ('nbsp', 8676), ('।', 7377), (',', 4054), ('.', 3470), ('-', 3039), ('\"', 2681), ('है', 2478), ('के', 2244)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('-', 8830), ('।', 7823), (';', 7577), (',', 5188), (':', 4110), ('&', 4064), ('\\n', 4011), ('nbsp', 3902), ('.', 3136), ('के', 2880)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('-', 13744), (';', 11740), ('।', 9153), (':', 6323), ('\\n', 6233), ('&', 6036), ('nbsp', 5789), (',', 5162), ('के', 3934), ('.', 3771)]\n",
      "xxxxxxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "for each in reading_levels:\n",
    "    print(all_counts[each].most_common(10))\n",
    "    print(\"xxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the first 10 words are pretty much the exact same amongst all 4 levels - this is because in any large text corpora, the stop words and punctutions and special signs will always rank highest in freqency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(';', 9882), ('&', 9251), ('nbsp', 9227), ('.', 3431), ('-', 2671), ('।', 2611), ('है', 1698), (',', 1644), ('!', 1180), ('\"', 997), ('के', 896), ('\\n', 815), ('ने', 780), ('से', 761), ('?', 735), ('और', 727), ('में', 685), ('को', 679), (':', 679), ('हैं', 671)]\n",
      "xxxxxxxxxxxxxx\n",
      "[(';', 9684), ('&', 8693), ('nbsp', 8676), ('।', 7377), (',', 4054), ('.', 3470), ('-', 3039), ('\"', 2681), ('है', 2478), ('के', 2244), ('और', 1931), ('!', 1915), ('से', 1895), ('में', 1871), ('ने', 1641), ('को', 1561), ('\\n', 1472), ('की', 1446), (':', 1349), ('हैं', 1233)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('-', 8830), ('।', 7823), (';', 7577), (',', 5188), (':', 4110), ('&', 4064), ('\\n', 4011), ('nbsp', 3902), ('.', 3136), ('के', 2880), ('है', 2858), ('में', 2498), ('\"', 2456), ('font', 2292), ('और', 2253), ('से', 2224), ('की', 1984), ('mso', 1864), ('हैं', 1648), ('को', 1631)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('-', 13744), (';', 11740), ('।', 9153), (':', 6323), ('\\n', 6233), ('&', 6036), ('nbsp', 5789), (',', 5162), ('के', 3934), ('.', 3771), ('mso', 3459), ('font', 3365), ('में', 3013), ('से', 2665), ('की', 2660), ('\"', 2645), ('है', 2585), ('और', 2334), ('को', 1865), ('था', 1627)]\n",
      "xxxxxxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "for each in reading_levels:\n",
    "    print(all_counts[each].most_common(20))\n",
    "    print(\"xxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the corpora, as well as the nature of the Hindi language (lots of lexical items added for extra information (tha, hain, se, etc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('उस', 143), ('ही', 140), ('2', 138), ('कुछ', 137), ('हम', 137), ('दिया', 136), ('रहा', 133), ('घर', 131), ('ये', 130), ('देखा', 125)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('अब', 314), ('साथ', 312), ('mso', 307), ('माँ', 301), ('पास', 299), ('अपनी', 291), ('उसके', 290), ('फिर', 284), ('रहे', 276), ('family', 276)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('रहा', 320), ('मैं', 317), ('साथ', 317), ('तरह', 316), ('फिर', 312), ('अपनी', 310), ('1', 306), ('रही', 301), ('इस', 300), ('@', 299)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('मैं', 457), ('”', 456), ('रहा', 444), ('3', 442), ('इस', 426), ('कहा', 411), ('size', 399), ('रहे', 394), ('@', 390), ('साथ', 384)]\n",
      "xxxxxxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "for each in reading_levels:\n",
    "    print(all_counts[each].most_common(70)[60:])\n",
    "    print(\"xxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On initial analysis, it does not look like non-stop words make a difference, at least those that are higher infrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('अंकल', 4), ('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nएक', 4), ('पट्टी', 4), ('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nमैं', 4), ('हँसती', 4), ('टुक', 4), ('मूँछ', 4), ('स\\u200dे', 4), ('चिड़ियाघर', 4), ('इसी', 4)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('भई', 9), ('बैंगन', 9), ('छी', 9), ('बचने', 9), ('हाल', 9), ('गड़बड़', 9), ('ऐनक', 9), ('आपका', 9), ('जमा', 9), ('the', 9)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('टुकड़ा', 10), ('छा', 10), ('अणुओं', 10), ('परत', 10), ('अणु', 10), ('ख़याल', 10), ('वर्ष', 10), ('घूम', 10), ('टूट', 10), ('मना', 10)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('Paragraph\\\\', 12), ('रुप', 12), ('जीत', 12), ('चलाने', 12), ('स्तर', 12), ('लंबी', 12), ('शोला', 12), ('गुब्बारा', 12), ('प्रदूषण', 12), ('बालू', 12)]\n",
      "xxxxxxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "for each in reading_levels:\n",
    "    print(all_counts[each].most_common(1600)[1590:])\n",
    "    print(\"xxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with mediocre-frequency words. Lets move on to the cream of the language - the words with unit-frequency. As can be seen with the trend of the word-frequency, this data and it's frequency very much holds with Zipf's law too (https://simple.wikipedia.org/wiki/Zipf%27s_law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7587"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_counts['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('लगाये', 1), ('घुमाये', 1), ('मेगी', 1), ('\\n\\nमुस्कराती', 1), ('भी\\n\\nमुस्कराते', 1), ('\\nम्याऊँ', 1), ('सूची1', 1), ('बड़ा2', 1), ('नीचे3', 1), ('चमकीला4', 1)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('\\n\\nपेंसिल', 1), ('दिखला', 1), ('\\n\\nसकती', 1), ('है\\n\\n\\n\\nकई', 1), ('\\n\\nपत्ते', 1), ('गमले', 1), ('\\n\\nरेखा', 1), ('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nरेल', 1), ('चली\\n\\n', 1), ('\\n\\nछुक', 1)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('छोटे\\nबच्चे', 1), ('पकड़ाए', 1), ('अब\\nसब', 1), ('\\n\\nरोज़', 1), ('कुछ\\nकाम', 1), ('बढ़ता\\nही', 1), ('योग्यता', 1), ('समाते', 1), ('बजे\\nउसे', 1), ('पतंगें\\nमायूस', 1)]\n",
      "xxxxxxxxxxxxxx\n",
      "[('छोड़ना', 2), ('पारदर्शी', 2), ('अपमानित', 2), ('\\n\\nसर्प', 2), ('सिखाना', 2), ('सप्ताह', 2), ('पूर्णमासी', 2), ('खोए', 2), ('हटते', 2), ('बचाई', 2)]\n",
      "xxxxxxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "for each in reading_levels:\n",
    "    print(all_counts[each].most_common(7000)[6990:])\n",
    "    print(\"xxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in difficulty of words can be seen here, with words of lower frequency. From world knowledge, it can be said that words like 'मुस्कराती' are used way more commonly than words like 'अपमानित'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
